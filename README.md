## DeepMind

This project utilizes Flutter for cross-platform mobile development, enabling seamless performance on both Android and iOS devices with a single codebase. The app integrates Firebase for secure backend services such as authentication, cloud storage, and real-time updates — making it scalable and reliable for future enhancements. For sign language detection, a TensorFlow/Keras machine learning model trained on thousands of ASL images provides accurate gesture recognition, while MediaPipe ensures robust hand tracking. A lightweight Flask API connects the frontend with the model to enable real-time predictions. These tools were chosen for their efficiency, accessibility, and open-source support, allowing the app to be developed rapidly and deployed widely. By leveraging these technologies, this app empowers individuals to learn sign language anytime, anywhere — helping bridge communication gaps and reduce inequalities faced by the Deaf and Hard of Hearing community.
























